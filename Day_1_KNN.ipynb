{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbC6bkLyj-Bx"
   },
   "source": [
    "# Day 1: Machine Learning Basics and Classification\n",
    "We will use Numpy for many matrix and array operations. Its user guide is available [here](https://numpy.org/doc/stable/user/basics.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXe5MUBEj-Bz"
   },
   "outputs": [],
   "source": [
    "# Import some libraries required later.\n",
    "import numpy as np\n",
    "\n",
    "# Set notebook to display matplotlib as an image. In Python it would usually be\n",
    "# a popup window, which does not work in a notebook.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsGrpR9Hj-B0"
   },
   "source": [
    "## Loading MNIST\n",
    "\n",
    "We first need a dataset. We will use the MNIST Digits dataset for this lab. We can load the dataset using [`load_digits`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html) from [`scikit-learn`](https://scikit-learn.org/stable/user_guide.html).\n",
    "\n",
    "Let's load the dataset into `digits`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3e0gmy5bj-B0"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgZb7dB35Zym"
   },
   "source": [
    "Now to see some data about the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_-gFX97J5d4n"
   },
   "outputs": [],
   "source": [
    "print('Data type:', type(digits))\n",
    "print('Description:', digits['DESCR'])\n",
    "print('Dataset shape (data):', digits.data.shape)\n",
    "print('Dataset shape (targets):', digits.target.shape)\n",
    "print(f'Data range: min {np.min(digits.data)}, max {np.max(digits.data)}')\n",
    "print(f'Target range: min {np.min(digits.target)}, max {np.max(digits.target)}')\n",
    "print('First sample is of type', type(digits.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXvjj8NEYSoU"
   },
   "source": [
    "**Question 1:** How many samples are in the dataset?\n",
    "\n",
    "**Question 2:** What does the dataset contain?\n",
    "\n",
    "Let's take a look at some of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AR03jkJh6CEI"
   },
   "outputs": [],
   "source": [
    "print('Sample 1:', digits.data[0])\n",
    "print('Sample 1 shape:', digits.data[0].shape)\n",
    "print('Sample 1 belongs to class', digits.target[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8FnYJSZX_wx"
   },
   "source": [
    "**Question 3:** What does a shape of `(64,)` mean?\n",
    "\n",
    "And to visualise this sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25aBsgeLWhQ5"
   },
   "outputs": [],
   "source": [
    "plt.imshow(digits.data[0].reshape(8, 8), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mM133pg5XMsX"
   },
   "source": [
    "**Question 4:** What did calling `.reshape(8,8)` on the sample do?\n",
    "\n",
    "**Question 5:** Why is the minimum of `digits.target` 0 and the maximum 9?\n",
    "\n",
    "Now split the dataset into 80% training and 20% testing sets using the [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) utility function. This function also shuffles the dataset...\n",
    "\n",
    "It is important to shuffle your dataset if it has been sorted. If you don't, your training-testing split won't accurately represent the dataset's true distribution. For example, if the samples were in order from 0 to 9, your training set (first 80%) may not include any examples of the digit 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KAQJNVUYj-B1"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# test_size is the proportion of the dataset to be returned in the testing set.\n",
    "# random_state forces the shuffling to be the same across runs.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=42)\n",
    "\n",
    "print('Training dataset samples:', X_train.shape)\n",
    "print('Training dataset targets:', y_train.shape)\n",
    "print('Testing dataset samples:', X_test.shape)\n",
    "print('Testing dataset targets:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yi2zYuskj-B1"
   },
   "source": [
    "## kNN classifier\n",
    "\n",
    "\n",
    "Let's simplify the problem for now and only try to identify one digit—for example, the number 5. This '5-detector' will be an example of a binary classifier, capable of distinguishing between just two classes, 5 and not-5. Let's create the target vectors for this classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JkYfBiu2j-B2"
   },
   "outputs": [],
   "source": [
    "# Create arrays of boolean values where the corresponding element in \n",
    "# y_train/y_test equals 5.\n",
    "y_train_5 = (y_train == 5)\n",
    "y_test_5 = (y_test == 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDqIQn2H3QO0"
   },
   "source": [
    " Now let's create and train the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CgA-96Bt3PZQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_clf = KNeighborsClassifier(weights='distance', n_neighbors=4)\n",
    "knn_clf.fit(X_train, y_train_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVI-Deez1bq1"
   },
   "source": [
    "And now test it with some inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l5z9bkCNj-B2"
   },
   "outputs": [],
   "source": [
    "rows, cols = 2, 5\n",
    "fig, ax = plt.subplots(rows, cols)\n",
    "ax = ax.flatten()\n",
    "for num, sample in enumerate(X_test[:rows*cols]):\n",
    "    ax[num].imshow(sample.reshape(8, 8), cmap='gray')\n",
    "    ax[num].set_title('Is a 5' if knn_clf.predict([sample]) else 'Not a 5')\n",
    "    ax[num].set_axis_off()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CarfJHv9j-B3"
   },
   "source": [
    "## Performance Measures\n",
    "\n",
    "Evaluating a classifier is often significantly trickier than evaluating a regressor, so we will spend a large part of this chapter on this topic. There are many performance measures available...\n",
    "\n",
    "A good way to evaluate a model is to use cross-validation. Let's use the [`cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html?highlight=cross_val_score#sklearn.model_selection.cross_val_score) function to evaluate your kNN model using k-fold cross-validation, with three folds (k=3). Remember that k-fold cross-validation means splitting the training set into k-folds (in this case, three), then making predictions and evaluating them on each fold using a model trained on the remaining folds.\n",
    "\n",
    "### Measuring Accuracy Using Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZkMzJTpj-B3"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(cross_val_score(knn_clf, X_train, y_train_5, cv=4, scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01g2mcrlj-B3"
   },
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "A much better way to evaluate the performance of a classifier is to look at the confusion matrix. The general idea is to count the number of times instances of class A are classified as class B. For example, to know the number of times the classifier confused images of 5s with 3s, you would look in the 5th row and 3rd column of the confusion matrix.\n",
    "\n",
    "To compute the confusion matrix, you first need to have a set of predictions, so they can be compared to the actual targets. You could make predictions on the test set, but let’s keep it untouched for now (remember that you want to use the test set only at the very end of your project, once you have a classifier that you are ready to launch). Instead, you can use the [`cross_val_predict`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html?highlight=cross_val_predict) function. Just like the `cross_val_score` function, `cross_val_predict` performs k-fold cross-validation, but instead of returning the evaluation scores, it returns the predictions made on each test fold. This means that you get a clean prediction for each instance in the training set ('clean' meaning that the prediction is made by a model that never saw the data during training).\n",
    "\n",
    "Now you are ready to get the confusion matrix using the [`confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html?highlight=confusion_matrix) function. Just pass it the target classes (`y_train_5`) and the predicted classes (`y_train_pred`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vkYeC44-j-B4"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_train_pred = cross_val_predict(knn_clf, X_train, y_train_5, cv=3)\n",
    "print(confusion_matrix(y_train_5, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxliwRbEj-B4"
   },
   "source": [
    "Each row in a confusion matrix represents an actual class, while each column represents a predicted class. The first row of this matrix considers non-5 images (the negative class): 1,301 of them were correctly classified as non-5s (they are called true negatives), while the remaining 1 was wrongly classified as 5s (false positives). The second row considers the images of 5s (the positive class): 4 were wrongly classified as non-5s (false negatives), while the remaining 131 were correctly classified as 5s (true positives). A perfect classifier would have only true positives and true negatives, so its confusion matrix would have nonzero values only on its main diagonal (top left to bottom right)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UTzmkbWj-B5"
   },
   "source": [
    "### Precision and Recall\n",
    "\n",
    "The confusion matrix gives you a lot of information, but sometimes you may prefer a more concise metric. An interesting one to look at is the accuracy of the positive predictions; this is called the precision of the classifier:\n",
    "\n",
    "\\begin{equation} \n",
    "precision = \\frac{TP}{TP+FP}\n",
    "\\end{equation}\n",
    "\n",
    "TP is the number of true positives, and FP is the number of false positives.\n",
    "\n",
    "A trivial way to have perfect precision is to make one single positive prediction and ensure it is correct (precision = 1/1 = 100%). This would not be very useful since the classifier would ignore all but one positive instance. So precision is typically used along with another metric named recall, also called sensitivity or true positive rate (TPR): this is the ratio of positive instances that are correctly detected by the classifier:\n",
    "\n",
    "\\begin{equation} \n",
    "recall = \\frac{TP}{TP+FN}\n",
    "\\end{equation}\n",
    "\n",
    "FN is of course the number of false negatives.\n",
    "\n",
    "We can use [`precision_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html?highlight=precision%20score#sklearn.metrics.precision_score) and [`recall_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html?highlight=recall%20score#sklearn.metrics.recall_score) from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lNbMXqzuj-B5"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "print(precision_score(y_train_5, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZ65h2E9j-B6"
   },
   "outputs": [],
   "source": [
    "# Your code here: compute the recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-iSaZ1uj-B6"
   },
   "source": [
    "It is often convenient to combine precision and recall into a single metric called the F1 score, in particular if you need a simple way to compare two classifiers. The F1 score is the harmonic mean of precision and recall. Whereas the regular mean treats all values equally, the harmonic mean gives much more weight to low values. As a result, the classifier will only get a high F1 score if both recall and precision are high.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation} \n",
    "F1 = 2 \\times \\frac{precision \\times recall}{precision+recall}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ExnhgMl0j-B6"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "# Your code here: compute the f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGkhsnYzj-B7"
   },
   "source": [
    "The F1 score favours classifiers that have similar precision and recall. This is not always what you want: in some contexts you mostly care about precision, and in other contexts you really care about recall. For example, if you trained a classifier to detect videos that are safe for kids, you would probably prefer a classifier that rejects many good videos (low recall) but keeps only safe ones (high precision), rather than a classifier that has a much higher recall but lets a few really bad videos show up in your product (in such cases, you may even want to add a human pipeline to check the classifier’s video selection). On the other hand, suppose you train a classifier to detect shoplifters on surveillance images: it is probably fine if your classifier has only 30% precision as long as it has 99% recall (sure, the security guards will get a few false alerts, but almost all shoplifters will get caught).\n",
    "\n",
    "Unfortunately, you can’t have it both ways: increasing precision reduces recall, and vice versa. This is called the precision/recall tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_v0JIXGj-B7"
   },
   "source": [
    "### ROC curves\n",
    "\n",
    "The receiver operating characteristic (ROC) curve is another common tool used with binary classifiers. It is very similar to the precision/recall curve, but instead of plotting precision versus recall, the ROC curve plots the true positive rate (another name for recall) against the false positive rate. The FPR is the ratio of negative instances that are incorrectly classified as positive. It is equal to one minus the true negative rate, which is the ratio of negative instances that are correctly classified as negative. The TNR is also called specificity. Hence the ROC curve plots sensitivity (recall) versus 1 – specificity.\n",
    "\n",
    "To plot the ROC curve, you first need to compute the TPR and FPR for various threshold values, using the roc_curve() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2a-iXFgj-B7"
   },
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QDLDZOwfj-B8"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "y_probas_knn = cross_val_predict(knn_clf, X_train, y_train_5, cv=3,\n",
    "                                 method='predict_proba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_tasr11j-B8"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "y_scores_knn = y_probas_knn[:, 1]  # score = proba of positive class\n",
    "fpr_knn, tpr_knn, thresholds_knn = roc_curve(y_train_5, y_scores_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKY2t-HVj-B8"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plot_roc_curve(fpr_knn, tpr_knn, 'kNN')\n",
    "plt.legend(loc='lower right', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHTVCD45j-B8"
   },
   "source": [
    "Once again there is a tradeoff: the higher the recall (TPR), the more false positives (FPR) the classifier produces. The dotted line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner).\n",
    "\n",
    "One way to compare classifiers is to measure the area under the curve (AUC). A perfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5. Scikit-Learn provides a function to compute the ROC AUC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JUOZwXBKj-B9"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_train_5, y_scores_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iUMJtNnUj-B9"
   },
   "source": [
    "## Multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_3P3AUdj-B9"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf = KNeighborsClassifier(weights='distance', n_neighbors=4)\n",
    "knn_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bONaaH4Bj-B9"
   },
   "outputs": [],
   "source": [
    "y_knn_pred = knn_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qG3pVkfjj-B-"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_knn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X3RHaJkUj-B-"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_train_pred = cross_val_predict(knn_clf, X_train, y_train, cv=3)\n",
    "conf_mx = confusion_matrix(y_train, y_train_pred)\n",
    "print(conf_mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38hwIrIBj-B-"
   },
   "outputs": [],
   "source": [
    "plt.matshow(conf_mx, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKUH8vh-j-B_"
   },
   "source": [
    "## Exercises\n",
    "\n",
    "**Task 1:** Try to use the different *folds* of k-fold cross-validation to evaluate your kNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lfXP1fDKwVrQ"
   },
   "outputs": [],
   "source": [
    "# change the \"cv\" parameter in the cross_val_predict function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gfAXB-oj-B_"
   },
   "source": [
    "**Task 2:** Try to use the different *n_neighbors* of kNN for the MNIST dataset.\n",
    "\n",
    "Then evaluate them (try  Confusion Matrix,  Precision,  Recall and ROC Curves). You can try to plot multiple ROC Curves into one figure (Hints: https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GGJdDq9rwZyQ"
   },
   "outputs": [],
   "source": [
    "# change 'n_neighbors' in KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvDqe_ffj-B_"
   },
   "source": [
    "**Task 3:** A more challenging exercise: Try to choose optimal value of *n_neighbors* of KNN with the help of cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0NqiHKi52lR"
   },
   "outputs": [],
   "source": [
    "# choose optimal value of n_neighbors of KNN with the help of cross validation"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Week_1_ML.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
